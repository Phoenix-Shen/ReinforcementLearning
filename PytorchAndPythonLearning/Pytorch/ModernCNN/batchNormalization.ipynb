{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批标准化 BatchNormalization\n",
    "\n",
    "batchNormalization 批标准化，与普通的数据标准化相似，是将分散的数据统一的一种做法，也是优化神经网络的一种方法，因为具有统一规格的数据能够让机器学习更容易学习到其中的规律\n",
    "\n",
    "## BN解决了ICS的问题吗？\n",
    "ICS = Internal Covariate Shift\n",
    "解决ICS的最基本办法就是对网络的输入作归一化，使得输入分布的均值为0，标准差为1，这个方法仅仅在网络不深的时候才奏效，网络一旦变深，每层参数引起的分布的微小变化叠加起来的变化是巨大的，所以我们需要对每一层的输入都做一些归一化操作。\n",
    "\n",
    "从整体上来看，BN层的作用就是通过参数控制了每一层输出的均值和方差。\n",
    "\n",
    "ICS问题是训练过程中，网络中间层输入分布的变化，BN层并没有解决ICS问题，而是引入了参数gamma和beta去调节中间层输出的均值和标准差，gamma和beta会在训练过程中不断更新，意味着均值和标准差也不断在变化，即BN本质上暗含了ICS。\n",
    "\n",
    "实际上，BN能够提高收敛速度\n",
    "\n",
    "## 使用BN层需要注意的细节\n",
    "1. BN层是放在激活层之后的\n",
    "2. 推理过程中没有batch情况下的主流解决方法：在训练时跟踪记录每一个batch的mean和var，然后使用这些值对全部样本的均值和标准差作无偏估计。Pytorch中model.eval()就是如此了\n",
    "3. BN层的正则化作用：在BN层中每个batch计算得到的均值和方差都是对于全局的一个近似估计，这为我们最优解的搜索引入了随机性，从而起到了正则化的作用\n",
    "4. BN的缺陷：带有BN层的网络错误率会随着batch_size的减小而迅速增大，当我们硬件条件受到限制不得不使用较小的batch_size的时候，网络的效果会大打折扣，后面LN、IN和GroupNormalization aka GN 是为了解决该问题的\n",
    "\n",
    "\n",
    "\n",
    "在神经网络中，数据分布会对训练产生影响\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9999) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 有时候激励函数对于变化范围较大的数据不敏感\n",
    "import torch as t\n",
    "x1 = t.tensor(5,dtype=t.float32)\n",
    "x2 = t.tensor(40,dtype=t.float32)\n",
    "\n",
    "print(t.tanh(x1),t.tanh(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，tanh(5)约等于tanh(40),这是很糟糕的，所以要在层与层之间把数据处理一下，让他们都在激活函数的敏感区间之内。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练深层神经网络十分困难，特别是我们的目标是要在较短时间内使它收敛的时候，BN是一种有效的技术，可持续加速深层网络的收敛速度。\n",
    "\n",
    "从形式上来说，BN根据以下表达式转换输入X\n",
    "\n",
    "$$\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$\n",
    "\n",
    "其中拉伸参数gamma和偏移参数beta他们的形状与输入X相同，但是是属于nn.parameters的，需要被学习\n",
    "\n",
    "## ***注意：在有BN层的网络中，选择合适的batch size比没有BN层的网络更加重要***\n",
    "\n",
    "# 从头开始实现BN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "def batch_norm(X:Tensor,\n",
    "                gamma:Tensor,\n",
    "                beta:Tensor,\n",
    "                moving_mean:Tensor,\n",
    "                moving_var:Tensor,\n",
    "                eps:float,\n",
    "                momentum:float)->tuple[Tensor,Tensor,Tensor]:\n",
    "    # 通过is_grad_enabled 来判断当前模式是训练模式还是预测模式\n",
    "    if not t.is_grad_enabled():\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat =(X-moving_mean)/t.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)\n",
    "        \n",
    "        # 如果使用全连接层，计算特征维上的均值和方差\n",
    "        if len(X.shape)==2:\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X-mean)**2).mean(dim = 0)\n",
    "        # 如果使用卷积层，计算通道维上的均值和方差,即所有的某通道的数值和除以（行乘以列乘以batch数）\n",
    "        else:\n",
    "            mean = X.mean(dim=(0,2,3),keepdim=True)\n",
    "            var = ((X-mean)**2).mean(dim = (0,2,3),keepdim=True)\n",
    "        \n",
    "        # 训练模式下，用当前的均值和方差做标准化\n",
    "        X_hat =(X-mean)/t.sqrt(var+eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum*moving_mean +(1-momentum)*mean\n",
    "        moving_var = momentum*moving_var +(1-momentum)*var\n",
    "\n",
    "    # 缩放和移位\n",
    "    Y=gamma*X_hat+beta\n",
    "    return Y,moving_mean.data,moving_var.data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义BN模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_features:int,num_dims:int)->None:\n",
    "        super().__init__()\n",
    "\n",
    "        if num_dims==2:\n",
    "            shape =(1,num_features)\n",
    "        else:\n",
    "            shape = (1,num_features,1,1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = nn.Parameter(t.ones(shape))\n",
    "        self.beta =nn.Parameter(t.zeros(shape))\n",
    "        # 非模型参数的变量初始化为0和1\n",
    "        self.moving_mean = t.zeros(shape)\n",
    "        self.moving_var = t.ones(shape )\n",
    "\n",
    "    def forward(self,X:Tensor)->Tensor:\n",
    "        if self.moving_mean.device!=X.device:\n",
    "            self.moving_mean=self.moving_mean.to(X.device)\n",
    "            self.moving_var=self.moving_var.to(X.device)\n",
    "        #保存更新之后的moving_mean和moving_var\n",
    "        Y,self.moving_mean,self.moving_var=batch_norm(X,self.gamma,self.beta,self.moving_mean,self.moving_var,eps=1e-5,momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "批量规范化被认为可以使优化更加的平滑，但是有时候会表现出论文中相反的结果\n",
    "\n",
    "批量规范化已经被证明是一种不可或缺的方法，它适用于几乎所有的图像分类器，并在学术界获得了数万引用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 关于 torch.Tensor.mean((0,2,3))的详细解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numerical calculations\n",
    "\n",
    "import torch as t\n",
    "\n",
    "X = t.arange(0,27,1,dtype=t.float32).reshape(1,3,3,3)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.,  2.],\n",
       "          [ 3.,  4.,  5.],\n",
       "          [ 6.,  7.,  8.]],\n",
       "\n",
       "         [[ 9., 10., 11.],\n",
       "          [12., 13., 14.],\n",
       "          [15., 16., 17.]],\n",
       "\n",
       "         [[18., 19., 20.],\n",
       "          [21., 22., 23.],\n",
       "          [24., 25., 26.]]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  1.,  2.],\n",
       "         [ 3.,  4.,  5.],\n",
       "         [ 6.,  7.,  8.]],\n",
       "\n",
       "        [[ 9., 10., 11.],\n",
       "         [12., 13., 14.],\n",
       "         [15., 16., 17.]],\n",
       "\n",
       "        [[18., 19., 20.],\n",
       "         [21., 22., 23.],\n",
       "         [24., 25., 26.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y= X.mean(0)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  4.,  5.],\n",
       "        [12., 13., 14.],\n",
       "        [21., 22., 23.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = Y.mean(1)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., 13., 22.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O = Z.mean(1)\n",
    "O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4., 13., 22.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# equals this\n",
    "X.mean((0,2,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# batch normalization 的Pytorch 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "bn = nn.BatchNorm2d(3)\n",
    "X = t.arange(0, 27, 1, dtype=t.float32).reshape(1, 3, 3, 3)\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight Parameter containing:\n",
      "tensor([1., 1., 1.], requires_grad=True)\n",
      "bias Parameter containing:\n",
      "tensor([0., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for name,param in bn.named_parameters():\n",
    "    print(name, param)\n",
    "# 三个通道，所以gamma 和 beta 均有三个参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.5492, -1.1619, -0.7746],\n",
       "          [-0.3873,  0.0000,  0.3873],\n",
       "          [ 0.7746,  1.1619,  1.5492]],\n",
       "\n",
       "         [[-1.5492, -1.1619, -0.7746],\n",
       "          [-0.3873,  0.0000,  0.3873],\n",
       "          [ 0.7746,  1.1619,  1.5492]],\n",
       "\n",
       "         [[-1.5492, -1.1619, -0.7746],\n",
       "          [-0.3873,  0.0000,  0.3873],\n",
       "          [ 0.7746,  1.1619,  1.5492]]]], grad_fn=<NativeBatchNormBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.5492, -1.1619, -0.7746],\n",
       "          [-0.3873,  0.0000,  0.3873],\n",
       "          [ 0.7746,  1.1619,  1.5492]],\n",
       "\n",
       "         [[-1.5492, -1.1619, -0.7746],\n",
       "          [-0.3873,  0.0000,  0.3873],\n",
       "          [ 0.7746,  1.1619,  1.5492]],\n",
       "\n",
       "         [[-1.5492, -1.1619, -0.7746],\n",
       "          [-0.3873,  0.0000,  0.3873],\n",
       "          [ 0.7746,  1.1619,  1.5492]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 来看看我们自己的bn是不是结果一样？\n",
    "bn2 = BatchNorm(3,4)\n",
    "bn2.forward(X)\n",
    "# 雀氏是一样的↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看nn.BatchNorm2d的参数 running mean 和 running var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4000, 1.3000, 2.2000])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.running_mean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.6500, 1.6500, 1.6500])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.running_var\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
