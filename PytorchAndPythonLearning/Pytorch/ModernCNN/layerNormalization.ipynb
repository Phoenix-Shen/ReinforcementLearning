{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LayerNormalization\n",
    "\n",
    "LN的提出，是为了解决BN的两个问题\n",
    "1. 不适合RNN这种动态网络，也就是每个batch中的数据，长度都不是固定的。\n",
    "2. batch_size变小模型性能急剧下降\n",
    "\n",
    "## 具体做法\n",
    "根据样本的特征数做归一化，也就是说 B*C*F的数据，根据C和F做归一化\n",
    "\n",
    "## 注意事项\n",
    "在CNN中，BN的性能还是比LN要好，但是最近很多人将自然语言领域的模型用来处理图像，所以还是会涉及到LN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型优化之Layer Normalization\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "def layer_norm(X:Tensor,\n",
    "                    gamma:Tensor,\n",
    "                    beta:Tensor,\n",
    "                    eps:float,\n",
    "                    )->tuple[Tensor,Tensor,Tensor]:\n",
    "    \n",
    "    \n",
    "    assert len(X.shape) == 4 \n",
    "    mean  = X.mean(dim=(1,2,3),keepdim=True)\n",
    "    var = ((X-mean)**2).mean(dim=(1, 2, 3), keepdim=True)\n",
    "\n",
    "\n",
    "    X_hat = (X-mean)/t.sqrt(var+eps)\n",
    "    # 进行缩放和移位，即乘以gamma加上beta\n",
    "    gamma =gamma.unsqueeze(0).repeat(X_hat.shape[0],1,1,1)\n",
    "    beta =beta.unsqueeze(0).repeat(X_hat.shape[0],1,1,1)\n",
    "    Y = X_hat*gamma+beta\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normal_shape: tuple[int], eps=1e-5) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.normal_shape = normal_shape\n",
    "        # 参与求梯度和迭代的拉伸和偏移 scale and center，分别初始化为1和0\n",
    "        self.gamma = nn.Parameter(t.ones(normal_shape))\n",
    "        self.beta = nn.Parameter(t.zeros(normal_shape))\n",
    "        # 非模型参数的变量初始化为0和1\n",
    "\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        # 保存更新之后的moving_mean和moving_var\n",
    "        Y= layer_norm(\n",
    "            X, self.gamma, self.beta, self.eps)\n",
    "        return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=t.randn((2,3,3,3))\n",
    "ln1=LayerNorm([3,3,3])\n",
    "ln2 = nn.LayerNorm([3, 3, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.0675,  0.6669, -2.7354],\n",
       "          [ 0.6440,  1.2522,  0.2103],\n",
       "          [ 0.1085,  0.6687, -0.8088]],\n",
       "\n",
       "         [[-1.0286,  1.1523, -0.7071],\n",
       "          [ 0.3461,  1.0671, -0.4448],\n",
       "          [ 0.6964,  0.8086, -0.5977]],\n",
       "\n",
       "         [[-0.9233,  0.6457, -0.6459],\n",
       "          [-0.9472,  0.4827,  1.2731],\n",
       "          [ 0.5590,  0.8756, -0.5509]]],\n",
       "\n",
       "\n",
       "        [[[-1.4413,  2.6829, -0.1880],\n",
       "          [ 0.9779,  1.3153, -1.8779],\n",
       "          [ 0.0497,  0.4556, -0.7516]],\n",
       "\n",
       "         [[ 0.8969,  0.4723, -0.1573],\n",
       "          [-0.7768,  0.6440, -1.3729],\n",
       "          [-0.3492,  0.4028, -0.5894]],\n",
       "\n",
       "         [[ 1.5562, -0.8485,  0.9283],\n",
       "          [-0.6250, -0.3974, -0.1383],\n",
       "          [ 0.1068, -1.2142,  0.2392]]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln1.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.0675,  0.6669, -2.7354],\n",
       "          [ 0.6440,  1.2522,  0.2103],\n",
       "          [ 0.1085,  0.6687, -0.8088]],\n",
       "\n",
       "         [[-1.0286,  1.1523, -0.7071],\n",
       "          [ 0.3461,  1.0671, -0.4448],\n",
       "          [ 0.6964,  0.8086, -0.5977]],\n",
       "\n",
       "         [[-0.9233,  0.6457, -0.6459],\n",
       "          [-0.9472,  0.4827,  1.2731],\n",
       "          [ 0.5590,  0.8756, -0.5509]]],\n",
       "\n",
       "\n",
       "        [[[-1.4413,  2.6829, -0.1880],\n",
       "          [ 0.9779,  1.3153, -1.8779],\n",
       "          [ 0.0497,  0.4556, -0.7516]],\n",
       "\n",
       "         [[ 0.8969,  0.4723, -0.1573],\n",
       "          [-0.7768,  0.6440, -1.3729],\n",
       "          [-0.3492,  0.4028, -0.5894]],\n",
       "\n",
       "         [[ 1.5562, -0.8485,  0.9283],\n",
       "          [-0.6250, -0.3974, -0.1383],\n",
       "          [ 0.1068, -1.2142,  0.2392]]]], grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ln2.forward(X)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
