{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 批标准化 BatchNormalization\n",
    "\n",
    "batchNormalization 批标准化，与普通的数据标准化相似，是将分散的数据统一的一种做法，也是优化神经网络的一种方法，因为具有统一规格的数据能够让机器学习更容易学习到其中的规律\n",
    "\n",
    "\n",
    "在神经网络中，数据分布会对训练产生影响\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9999) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# 有时候激励函数对于变化范围较大的数据不敏感\n",
    "import torch as t\n",
    "x1 = t.tensor(5,dtype=t.float32)\n",
    "x2 = t.tensor(40,dtype=t.float32)\n",
    "\n",
    "print(t.tanh(x1),t.tanh(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，tanh(5)约等于tanh(40),这是很糟糕的，所以要在层与层之间把数据处理一下，让他们都在激活函数的敏感区间之内。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练深层神经网络十分困难，特别是我们的目标是要在较短时间内使它收敛的时候，BN是一种有效的技术，可持续加速深层网络的收敛速度。\n",
    "\n",
    "从形式上来说，BN根据以下表达式转换输入X\n",
    "\n",
    "$$\\mathrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$\n",
    "\n",
    "其中拉伸参数gamma和偏移参数beta他们的形状与输入X相同，但是是属于nn.parameters的，需要被学习\n",
    "\n",
    "# 从头开始实现BN层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "def batch_norm(X:Tensor,gamma:Tensor,beta:Tensor,moving_mean:Tensor,moving_var:Tensor,eps:float,momentum:float)->tuple[Tensor,Tensor,Tensor]:\n",
    "    # 通过is_grad_enabled 来判断当前模式是训练模式还是预测模式\n",
    "    if not t.is_grad_enabled():\n",
    "        # 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差\n",
    "        X_hat =(X-moving_mean)/t.sqrt(moving_var+eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4)\n",
    "        \n",
    "        # 如果使用全连接层，计算特征维上的均值和方差\n",
    "        if len(X.shape)==2:\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X-mean)**2).mean(dim = 0)\n",
    "        # 如果使用卷积层，计算通道维上的均值和方差,即所有的某通道的数值和除以（行乘以列乘以batch数）\n",
    "        else:\n",
    "            mean = X.mean(dim=(0,2,3),keepdim=True)\n",
    "            var = ((X-mean)**2).mean(dim = (0,2,3),keepdim=True)\n",
    "        \n",
    "        # 训练模式下，用当前的均值和方差做标准化\n",
    "        X_hat =(X-mean)/t.sqrt(var+eps)\n",
    "        # 更新移动平均的均值和方差\n",
    "        moving_mean = momentum*moving_mean +(1-momentum)*mean\n",
    "        moving_var = momentum*moving_var +(1-momentum)*var\n",
    "\n",
    "    # 缩放和移位\n",
    "    Y=gamma*X_hat+beta\n",
    "    return Y,moving_mean.data,moving_var.data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自定义BN模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self,num_features:int,num_dims:int)->None:\n",
    "        super().__init__()\n",
    "\n",
    "        if num_dims==2:\n",
    "            shape =(1,num_features)\n",
    "        else:\n",
    "            shape = (1,num_features,1,1)\n",
    "        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0\n",
    "        self.gamma = nn.Parameter(t.ones(shape))\n",
    "        self.beta =nn.Parameter(t.zeros(shape))\n",
    "        # 非模型参数的变量初始化为0和1\n",
    "        self.moving_mean = t.zeros(shape)\n",
    "        self.moving_var = t.ones(shape )\n",
    "\n",
    "    def forward(self,X:Tensor)->Tensor:\n",
    "        if self.moving_mean.device!=X.device:\n",
    "            self.moving_mean=self.moving_mean.to(X.device)\n",
    "            self.moving_var=self.moving_var.to(X.device)\n",
    "        #保存更新之后的moving_mean和moving_var\n",
    "        Y,self.moving_mean,self.moving_var=batch_norm(X,self.gamma,self.beta,self.moving_mean,self.moving_var,eps=1e-5,momentum=0.9)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= t.tensor([[1.,2.,3.]])\n",
    "label = t.tensor([0])\n",
    "loss_func =t.nn.CrossEntropyLoss()\n",
    "loss_crossepy=loss_func(a,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4076)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_crossepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4076)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "#########################\n",
    "a_softmax = F.softmax(a,1)\n",
    "log_a = a_softmax.log()\n",
    "loss_nll = F.nll_loss(log_a,label)\n",
    "#########################\n",
    "loss_nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1554, 0.4223, 0.4223]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn.functional as F\n",
    "a =t.tensor([[0.,1.,1.]])\n",
    "label=t.tensor([2])\n",
    "a_softmax = F.softmax(a,1)\n",
    "a_softmax\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
