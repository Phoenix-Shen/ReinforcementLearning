{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout\n",
    "之前使用l2_penalty来正则化统计模型，当面对更多的特征而样本不足的时候，我们的线性模型往往会过拟合，如果我们能够给出更多样本，那么就不会 过拟合。泛化性和灵活性这种的基本权衡被称为方差-偏差权衡（bias-variance tradeoff），线性模型具有很高的偏差，他们只能够表示一小类函数，但是方差很低；神经网络属于另一端，偏差小，方差大。\n",
    "\n",
    "dropout就是指在深度学习网络训练的过程中，对于神经网络单元，按照一定的概率暂时从神经网络中丢弃，由于是随机丢弃，每个minibatch都是在训练不同的网络，dropout是cnn中防止过拟合的一个大杀器。\n",
    "## 什么是好的模型??\n",
    "我们可以说好的模型在未知的数据集上面往往有很好的表现，经典的泛化理论认为，为了缩小训练和测试性能之间的差距，我们应该以简单的模型为目标。\n",
    "\n",
    "简单性的另外一个角度是平滑性，我们的函数不应该对输入的微小变化敏感。\n",
    "\n",
    "## 扰动的稳健性\n",
    "在训练过程中，在计算后续层之前向网络的每一层注入噪声。\n",
    "\n",
    "这个想法被称为**暂退法(Dropout)**。暂退法在前向传播的过程中，计算每一内部层的同时注入噪声，这是训练神经网络常用的技术，之所以被称为暂退法，是因为从表面上看我们是在训练过程中随机丢弃（Dropout）一些神经元。\n",
    "\n",
    "## 为什么能够解决过拟合的问题呢？\n",
    "\n",
    "1. 取平均的作用\n",
    "    标准的模型没有dropout，我们训练五个神经网络，执行投票策略或者取平均，得到1个最终结果，这种综合起来取平均的策略可以防止过拟合，因为这种平均可能让几个网络的过拟合相互抵消。dropout掉不同的神经元就类似于在训练不同的网络，随机删掉神经元的操作导致网络结构不同，因此可以视为最后的网络是这些结构不同的网络的“平均”\n",
    "\n",
    "2. 减少神经元之间复杂的共适应关系\n",
    "    dropout导致两个神经元不一定每次都在一个网络中出现，这样权值的更新不再依赖于有固定关系的隐含节点的共同作用，阻止了某些特征仅仅在其他特定特征下才有的情况。迫使网络去学习更加鲁棒的特征。我们的神经网络在做某种预测，他不应该对于一些特定的线索片段太过于敏感，即使丢掉特定的线索，他也可以从众多其它线索中学习到一些共同的特征。有点像L1,L2正则，减少权重使得模型能够增加鲁棒性\n",
    "\n",
    "3. 类似于生物进化的角色\n",
    "    物种为了生存往往会倾向于适应这种环境，环境突变则会导致物种难以做出及时反应，性别的出现可以繁衍出适应新环境的变种，有效的阻止过拟合，即避免环境改变时物种可能面临的灭绝。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from pltutils import *\n",
    "DEVICE = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X:t.Tensor,dropout):\n",
    "    assert 0<=dropout<=1\n",
    "    if dropout==1:\n",
    "        return t.zeros_like(X)\n",
    "    if dropout==0:\n",
    "        return X\n",
    "    mask  = (t.rand(X.shape)>dropout).float()\n",
    "    return mask.to(DEVICE)*X/(1.0-dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15.], device='cuda:0')\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15.], device='cuda:0')\n",
      "tensor([ 0.,  2.,  0.,  6.,  0., 10., 12.,  0.,  0., 18.,  0., 22., 24., 26.,\n",
      "         0., 30.], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# TEST of dropout_layer func\n",
    "\n",
    "X=t.arange(16,dtype=t.float32,device=DEVICE)\n",
    "print(X)\n",
    "print(dropout_layer(X,0))\n",
    "print(dropout_layer(X, .5))\n",
    "\n",
    "print(dropout_layer(X, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self,n_features,n_outputs,training=True) -> None:\n",
    "        super().__init__()\n",
    "        self.training=training\n",
    "        self.n_features=n_features\n",
    "        self.fc1=nn.Linear(n_features,256)\n",
    "        self.fc2=nn.Linear(256,256)\n",
    "        self.fc3=nn.Linear(256,n_outputs)\n",
    "        self.relu=nn.ReLU()\n",
    "    \n",
    "    def forward(self,X:t.Tensor)->t.Tensor:\n",
    "        H1=self.relu(self.fc1(X.reshape((-1,self.n_features))))\n",
    "        if self.training:\n",
    "            H1=dropout_layer(H1,0.2)\n",
    "        H2=self.relu(self.fc2(H1))\n",
    "        if self.training:\n",
    "            H2=dropout_layer(H2,0.5)\n",
    "        out=self.fc3(H2)\n",
    "        return out\n",
    "\n",
    "\n",
    "def predict(net, test_iter):\n",
    "    if isinstance(net, nn.Module):\n",
    "\n",
    "        net.training = False\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Method or function hasn't been implemented yet\")\n",
    "    correct = 0.\n",
    "    for x, y in test_iter:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        y_hat = net(x)\n",
    "        correct += accuracy(y_hat, y)\n",
    "    correct = correct/10000.  # 10000是测试集的长度\n",
    "    #print(\"test ACC:{}\".format(correct))\n",
    "    return correct\n",
    "\n",
    "def accuracy(y_hat: t.Tensor, y: t.Tensor) -> t.Tensor:\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(dim=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "\n",
    "def train(net, train_iter: data.DataLoader,loss, updater, n_epochs=10):\n",
    "    from tensorboardX import SummaryWriter\n",
    "    sw = SummaryWriter(\"./logs\")\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for x, y in train_iter:\n",
    "            if isinstance(net, t.nn.Module):\n",
    "                net.training=True\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            y_hat = net(x)\n",
    "            l = loss(y_hat, y)\n",
    "\n",
    "            if isinstance(updater, t.optim.Optimizer):\n",
    "                updater.zero_grad()\n",
    "                l.sum().backward()\n",
    "                updater.step()\n",
    "            else:\n",
    "                l.sum().backward()\n",
    "                updater(x.shape[0])\n",
    "            \n",
    "            train_acc = accuracy(y_hat, y)/y.shape[0]\n",
    "            test_acc = predict(net,test_iter)\n",
    "            print(\"ep:{},train_acc:{},test_acc:{},loss:{}\".format(\n",
    "                i, train_acc, test_acc,l.mean().item()))\n",
    "            sw.add_scalar(\"train_acc\",train_acc,i)\n",
    "            sw.add_scalar(\"test_acc\",test_acc,i)\n",
    "\n",
    "# entity\n",
    "net = NET(784,10,True)\n",
    "net = net.to(DEVICE)\n",
    "NUM_EPOCHS,LR,BATCH_SIZE=10,0.07,256\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "train_iter,test_iter=load_data_fashion_mnist(BATCH_SIZE,data_root=\"./dataset\",n_threads=0)\n",
    "optimizer= t.optim.SGD(net.parameters(),lr=LR)\n",
    "train(net,train_iter,loss_func,optimizer,NUM_EPOCHS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8264"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predict(net, test_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras中的Dropout函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  2.  3.  4.  5.  6.  7.  8.  9. 10.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.       ,  3.3333333,  5.       ,  6.6666665,  8.333333 ,\n",
       "       10.       , 11.666666 , 13.333333 , 14.999999 , 16.666666 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def dropout(x:np.ndarray,level:float)->np.ndarray:\n",
    "    assert level<1.0 and level>=0\n",
    "\n",
    "    retain_prob = 1.-level\n",
    "\n",
    "    random_tensor = np.random.binomial(n=1,p=retain_prob,size=x.shape)\n",
    "    x*=random_tensor\n",
    "    print(x)\n",
    "    x/=retain_prob\n",
    "    return x\n",
    "\n",
    "\n",
    "x = np.asarray([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype=np.float32)\n",
    "dropout(x, 0.4)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
