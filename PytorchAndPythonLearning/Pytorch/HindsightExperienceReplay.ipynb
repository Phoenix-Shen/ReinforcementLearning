{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HER\n",
    "1. 事后诸葛,应用于稀疏奖励和二分奖励，不需要复杂的奖励函数工程设计。\n",
    "\n",
    "2. 强化学习中的一个最棘手的问题就是稀疏奖励：经验池子中很少包含有奖励的经验\n",
    "\n",
    "3. 于是我们有HER，可以从稀疏、二分奖励的问题中高效采样并进行学习，而且可以运用于***所有OFF policy算法中***\n",
    "\n",
    "4. 二分奖励\n",
    "    - 简单来说，就是完成目标是一个值，没有完成就是另外一个值\n",
    "    - if reach the goal : r = 0 else: r=-1\n",
    "\n",
    "5. 稀疏奖励\n",
    "  简言之，完成目标的episode太少或者完成目标的步数太长，导致负奖励的样本数过多\n",
    "\n",
    "6. 在机器人领域，要想使强化学习训练它完美执行某任务，往往需要设计合理的奖励函数，但是设计这样的奖励函数工程师不仅需要懂得强化学习的领域知识，也需要懂得机器人、运动学等领域的知识。而且，有这些知识也未必能设计出很好的奖励函数供智能体进行学习。因此，如果可以从简单的奖励函数（如二分奖励）学习到可完成任务的模型，那就不需要费心设计复杂的奖励函数了。\n",
    "\n",
    "7. 就是运用了这个思想对经验池进行了扩充，将稀疏奖励问题给转化成非稀疏奖励，大大的扩展了经验池中完成任务的经验数量"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
