{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 卷积\n",
    "对于图片来说，不适合使用MLP来处理，因为参数过多导致难以拟合，卷积神经网络Convolutional Neural Networks，是机器学习中利用一些已知结构的创造性方法。\n",
    "\n",
    "卷积神经网络正式将空间不变性这一概念系统化，基于这个模型使用较少的参数来学习有用的表示：\n",
    "\n",
    "inductive bias\n",
    "\n",
    "1. 平移不变性：图像经过平移，相应的特征图上的表达也是平移的\n",
    "2. 局部性：神经网络的前面几层应该只探索输入图像的局部区域，而不是过度在意图像中相隔较远区域的关系，最终可以聚合这些特征，以在整个图像级别进行预测\n",
    "\n",
    "\n",
    "# 平移不变性\n",
    "如果一幅图片是h*w*1的，姑且算是单通道，那么用MLP的话，要输出h'*w'*1的数据，我们需要h*w*h'*w'的数据，这将是爆炸的。\n",
    "如果利用平移不变性的话，至少可以将参数缩减到h*w\n",
    "\n",
    "# 局部性\n",
    "我们不应该偏离到很远的地方，那么还可以进一步缩小参数,可以将h*w缩小很多\n",
    "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
    "\n",
    "# 卷积\n",
    "在数学中，两个函数的卷积被定义为\n",
    "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$$\n",
    "\n",
    "卷积就是把一个函数“翻转”并位移X时，测量f和g之间的重叠，见\n",
    "https://www.zhihu.com/question/22298352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现coor2d函数，接受张量X和卷积核张量K，并返回张量Y\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def corr2d(X: Tensor, K: Tensor) -> Tensor:\n",
    "    \"\"\"计算二维互相关运算,len(X.shape)=len(K.shape)=2\"\"\"\n",
    "    h,w=K.shape\n",
    "    Y = t.zeros((X.shape[0]-h+1,X.shape[1]-w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j]=(X[i:i+h,j:j+w]*K).sum()\n",
    "    return Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 尝试计算一下？\n",
    "X = t.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K= t.tensor([[0.,1.],[2.,3.]])\n",
    "corr2d(X,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义卷积层\n",
    "class CONV2D(nn.Module):\n",
    "    def __init__(self,kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(t.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(t.zeros(1))\n",
    "\n",
    "    def forward(self,x:Tensor)->Tensor:\n",
    "        return corr2d(x,self.weight)+self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = t.ones((6,8))\n",
    "X[:,2:6]=0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=t.tensor([[1.,-1.]])\n",
    "Y=corr2d(X,K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转置之后，失去了其相应的判别能力\n",
    "corr2d(X.t(), K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 29.116\n",
      "epoch 2, loss 13.258\n",
      "epoch 3, loss 6.283\n",
      "epoch 4, loss 3.119\n",
      "epoch 5, loss 1.627\n",
      "epoch 6, loss 0.890\n",
      "epoch 7, loss 0.508\n",
      "epoch 8, loss 0.299\n",
      "epoch 9, loss 0.181\n",
      "epoch 10, loss 0.112\n"
     ]
    }
   ],
   "source": [
    "# 学习卷积核的参数\n",
    "conv2d =nn.Conv2d(1,1,(1,2),bias=False)\n",
    "\n",
    "X=X.reshape((1,1,6,8))\n",
    "Y= Y.reshape((1,1,6,7))\n",
    "lr = 3e-2\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat-Y)**2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    conv2d.weight.data[:]-=lr*conv2d.weight.grad\n",
    "    print(f'epoch {i+1}, loss {l.sum():.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[ 0.9506, -1.0173]]]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in conv2d.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 互相关和卷积\n",
    "他俩是不同的，卷积运算需要水平和垂直翻转卷积核张量，然后对输入张量执行互相关运算\n",
    "\n",
    "# 特征映射和感受野\n",
    "输出的卷积层有时候被称为“特征映射” feature map，对于某一层的任意元素X，其感受野receptive field 是指在前向传播期间可能影响X计算的所有元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  0.,  0.,  0.],\n",
       "        [-1.,  1.,  0.,  0.],\n",
       "        [ 0., -1.,  1.,  0.],\n",
       "        [ 0.,  0., -1.,  1.],\n",
       "        [ 0.,  0.,  0., -1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 练习\n",
    "X = t.eye(5)\n",
    "corr2d(X,K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1., -1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1., -1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1., -1.,  0.],\n",
       "        [ 0.,  0.,  0.,  1., -1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d(X,K.T)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
