{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多输入多输出通道\n",
    "在之前的卷积层中，我们经常会有Tensor.reshape(1,1,x,x)这种操作，第一个1是batch，第二个1是1维度，然而图片一般是收纳通道，例如256*256的图片，它的尺寸是3,256,256\n",
    "\n",
    "当输入包含多个通道的时候，我们要构造一个与输入数据具有相同输入通道数的卷积核，以便于输入数据进行互相关运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[[-0.0810,  0.2035],\n",
      "          [ 0.0982,  0.1462]],\n",
      "\n",
      "         [[ 0.1033, -0.2700],\n",
      "          [-0.2734, -0.1603]],\n",
      "\n",
      "         [[-0.2174, -0.1042],\n",
      "          [-0.0885,  0.0224]]],\n",
      "\n",
      "\n",
      "        [[[-0.1758,  0.1863],\n",
      "          [-0.0014, -0.1177]],\n",
      "\n",
      "         [[ 0.2787, -0.0716],\n",
      "          [ 0.1335, -0.2635]],\n",
      "\n",
      "         [[ 0.2304,  0.2245],\n",
      "          [-0.1260, -0.1318]]],\n",
      "\n",
      "\n",
      "        [[[-0.0032,  0.0122],\n",
      "          [ 0.2438,  0.1136]],\n",
      "\n",
      "         [[-0.0258,  0.1951],\n",
      "          [ 0.1731, -0.1432]],\n",
      "\n",
      "         [[-0.0817, -0.1634],\n",
      "          [ 0.0656,  0.0287]]]], requires_grad=True) torch.Size([3, 3, 2, 2])\n",
      "Parameter containing:\n",
      "tensor([-0.0071,  0.1748,  0.1445], requires_grad=True) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "conv = nn.Conv2d(3,3,2)\n",
    "\n",
    "for param in conv.parameters():\n",
    "    print(param,param.shape)\n",
    "# 我们可以看到参数的shape = torch.Size([3, 3, 2, 2])\n",
    "# 意思是3个outchannel 3个 inchannel 2*2的卷积核大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X: Tensor, K: Tensor) -> Tensor:\n",
    "    \"\"\"计算二维互相关运算,len(X.shape)=len(K.shape)=2\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = t.zeros((X.shape[0]-h+1, X.shape[1]-w+1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i+h, j:j+w]*K).sum()\n",
    "    return Y\n",
    "# 定义多输入通道互相关逆运算，对每个通道执行互相关操作\n",
    "def corr2d_multi_in(X:Tensor,K:Tensor)->Tensor:\n",
    "    return sum(corr2d(x,k) for x,k in zip(X,K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3]) torch.Size([2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面是一个2通道3*3图片的示例\n",
    "X = t.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "                  [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = t.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "print(X.shape,K.shape)\n",
    "corr2d_multi_in(X, K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多输出通道\n",
    "每一层我们都要保证有多个输出通道是至关重要的，在流行的神经网络中，随着网络层数的加深，我们常常会增加输出通道的维度，通过减少空间分辨率以获得更大的通道深度。\n",
    "\n",
    "由Ci和Co分别代表输入和输出通道的数目，并使用Kh和Kw来表示卷积核的高度和宽度，为了获得多个通道的输出，我们可以为每个输出通道创建一个形状为Ci\\*Kh\\*Kw的卷积核张量，有Co个通道，所以一共卷积核的形状是Co\\*Ci\\*Kh\\*Kw，*这就是我们刚刚看到的nn.Conv2d中的参数的意义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 实现多个通道的输入和输出的互相关函数\n",
    "def corr2d_multi_in_out(X:Tensor,K:Tensor)->Tensor:\n",
    "    return t.stack([corr2d_multi_in(X,k) for k in K],0)\n",
    "\n",
    "K = t.stack((K,K+1,K+2),0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_in_out(X,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1*1卷积层\n",
    "1*1卷积不能提取相关的特征，但是它可以提取通道的特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in_out_1x1(X:Tensor,K:Tensor)->Tensor:\n",
    "    c_i,h,w=X.shape\n",
    "    c_o=K.shape[0]\n",
    "    X=X.reshape((c_i,h*w))\n",
    "    K=K.reshape((c_o,c_i))\n",
    "    Y=t.matmul(K,X)\n",
    "    return Y.reshape((c_o,h,w))\n",
    "\n",
    "X = t.normal(0, 1, (3, 3, 3))\n",
    "K = t.normal(0, 1, (2, 3, 1, 1))\n",
    "\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(t.abs(Y1 - Y2).sum()) < 1e-6"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
