{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropout\n",
    "之前使用l2_penalty来正则化统计模型，当面对更多的特征而样本不足的时候，我们的线性模型往往会过拟合，如果我们能够给出更多样本，那么就不会 过拟合。泛化性和灵活性这种的基本权衡被称为方差-偏差权衡（bias-variance tradeoff），线性模型具有很高的偏差，他们只能够表示一小类函数，但是方差很低；神经网络属于另一端，偏差小，方差大。\n",
    "\n",
    "## 什么是好的模型??\n",
    "我们可以说好的模型在未知的数据集上面往往有很好的表现，经典的泛化理论认为，为了缩小训练和测试性能之间的差距，我们应该以简单的模型为目标。\n",
    "\n",
    "简单性的另外一个角度是平滑性，我们的函数不应该对输入的微小变化敏感。\n",
    "\n",
    "## 扰动的稳健性\n",
    "在训练过程中，在计算后续层之前向网络的每一层注入噪声。\n",
    "\n",
    "这个想法被称为**暂退法(Dropout)**。暂退法在前向传播的过程中，计算每一内部层的同时注入噪声，这是训练神经网络常用的技术，之所以被称为暂退法，是因为从表面上看我们是在训练过程中随机丢弃（Dropout）一些神经元。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from pltutils import *\n",
    "DEVICE = t.device(\"cuda:0\" if t.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X:t.Tensor,dropout):\n",
    "    assert 0<=dropout<=1\n",
    "    if dropout==1:\n",
    "        return t.zeros_like(X)\n",
    "    if dropout==0:\n",
    "        return X\n",
    "    mask  = (t.rand(X.shape)>dropout).float()\n",
    "    return mask.to(DEVICE)*X/(1.0-dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15.], device='cuda:0')\n",
      "tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n",
      "        14., 15.], device='cuda:0')\n",
      "tensor([ 0.,  2.,  0.,  6.,  0., 10., 12.,  0.,  0., 18.,  0., 22., 24., 26.,\n",
      "         0., 30.], device='cuda:0')\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# TEST of dropout_layer func\n",
    "\n",
    "X=t.arange(16,dtype=t.float32,device=DEVICE)\n",
    "print(X)\n",
    "print(dropout_layer(X,0))\n",
    "print(dropout_layer(X, .5))\n",
    "\n",
    "print(dropout_layer(X, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NET(nn.Module):\n",
    "    def __init__(self,n_features,n_outputs,training=True) -> None:\n",
    "        super().__init__()\n",
    "        self.training=training\n",
    "        self.n_features=n_features\n",
    "        self.fc1=nn.Linear(n_features,256)\n",
    "        self.fc2=nn.Linear(256,256)\n",
    "        self.fc3=nn.Linear(256,n_outputs)\n",
    "        self.relu=nn.ReLU()\n",
    "    \n",
    "    def forward(self,X:t.Tensor)->t.Tensor:\n",
    "        H1=self.relu(self.fc1(X.reshape((-1,self.n_features))))\n",
    "        if self.training:\n",
    "            H1=dropout_layer(H1,0.2)\n",
    "        H2=self.relu(self.fc2(H1))\n",
    "        if self.training:\n",
    "            H2=dropout_layer(H2,0.5)\n",
    "        out=self.fc3(H2)\n",
    "        return out\n",
    "\n",
    "\n",
    "def predict(net, test_iter):\n",
    "    if isinstance(net, nn.Module):\n",
    "\n",
    "        net.training = False\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            \"Method or function hasn't been implemented yet\")\n",
    "    correct = 0.\n",
    "    for x, y in test_iter:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        y_hat = net(x)\n",
    "        correct += accuracy(y_hat, y)\n",
    "    correct = correct/10000.  # 10000是测试集的长度\n",
    "    #print(\"test ACC:{}\".format(correct))\n",
    "    return correct\n",
    "\n",
    "def accuracy(y_hat: t.Tensor, y: t.Tensor) -> t.Tensor:\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(dim=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "\n",
    "def train(net, train_iter: data.DataLoader,loss, updater, n_epochs=10):\n",
    "    from tensorboardX import SummaryWriter\n",
    "    sw = SummaryWriter(\"./logs\")\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        for x, y in train_iter:\n",
    "            if isinstance(net, t.nn.Module):\n",
    "                net.training=True\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            y_hat = net(x)\n",
    "            l = loss(y_hat, y)\n",
    "\n",
    "            if isinstance(updater, t.optim.Optimizer):\n",
    "                updater.zero_grad()\n",
    "                l.sum().backward()\n",
    "                updater.step()\n",
    "            else:\n",
    "                l.sum().backward()\n",
    "                updater(x.shape[0])\n",
    "            \n",
    "            train_acc = accuracy(y_hat, y)/y.shape[0]\n",
    "            test_acc = predict(net,test_iter)\n",
    "            print(\"ep:{},train_acc:{},test_acc:{},loss:{}\".format(\n",
    "                i, train_acc, test_acc,l.mean().item()))\n",
    "            sw.add_scalar(\"train_acc\",train_acc,i)\n",
    "            sw.add_scalar(\"test_acc\",test_acc,i)\n",
    "\n",
    "# entity\n",
    "net = NET(784,10,True)\n",
    "net = net.to(DEVICE)\n",
    "NUM_EPOCHS,LR,BATCH_SIZE=10,0.07,256\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "train_iter,test_iter=load_data_fashion_mnist(BATCH_SIZE,data_root=\"./dataset\",n_threads=0)\n",
    "optimizer= t.optim.SGD(net.parameters(),lr=LR)\n",
    "train(net,train_iter,loss_func,optimizer,NUM_EPOCHS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8264"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "predict(net, test_iter)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32fe4a0c0b23bf2d0ff7b6ec889b7996b95e9e7ff48467869f67c8fd61e3e485"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
