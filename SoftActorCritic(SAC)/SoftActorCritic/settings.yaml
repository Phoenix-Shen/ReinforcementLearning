env_name: LunarLanderContinuous-v2

# NETWORK SETTINGS

# network's hidden size
hiddenlayer_size: 256
# learning rate of the actor and critic
# use 1.0e-4,don't use 1e-4 or it will be considered as string
lr_actor: 1.0e-4
lr_critic: 1.0e-4
# soft update coefficient
tau: 0.05

# TRAINING SETTINGS

# use cuda?
cuda: True
#the number of total epochs
n_episodes: 100000
# the steps of the initial exploration
init_exploration_steps: 10000
# reward scale, a common trick
reward_scale: 0.114514
# reward_decay
gamma: 0.99
# the max and min of log standard variance value
log_std_max: 1
log_std_min: -20
# entropy weights, it is the alpha in the paper but now they include the alpha as a parameter to be learnt,
# the value is none if you want the model tune the value automatically
entropy_weights: 0.1
# the interval to update the target network
target_update_interval: 5
# how many updates apply in the update
update_cycles: 5

# MEMORY SETTINGS
buffer_size: 1.0e+6
batch_size: 256

# EVALUATION SETTINGS
eval_interval: 100
eval_episodes: 10

# LOGS AND MODEL SAVING PATH

# where to store the tensorboardX logs?
log_dir: SoftActorCritic(SAC)/SoftActorCritic/logs
# the interval of saving the parameters of the actor and the critic
save_frequency: 5000
save_dir: SoftActorCritic(SAC)/SoftActorCritic/saved_models

##################
#predict settings#
##################
actor_model_dir: SoftActorCritic(SAC)\SoftActorCritic\saved_models\ACTOR 2022-01-12 22-17-40.pth
